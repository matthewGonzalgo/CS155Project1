{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data as torch_data\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.fillna(0)\n",
    "train_data = df.values\n",
    "X = train_data[:,1:27]\n",
    "y = train_data[:,27].astype(int)\n",
    "N = y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6430416286842905"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.sum(y) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(26, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,2),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, n_epochs, stop_thresh):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        num_folds = 5\n",
    "        kf = KFold(n_splits=num_folds)\n",
    "        sum_train_mse = 0\n",
    "        sum_val_mse = 0\n",
    "        \n",
    "        for train_index, val_index in kf.split(X[0:N]):\n",
    "            # Retreive training and test data\n",
    "            train_data_x, val_data_x = X[train_index], X[val_index]\n",
    "            train_data_y, val_data_y = y[train_index], y[val_index]\n",
    "            tensor_x = torch.Tensor(train_data_x) # transform to torch tensor\n",
    "            tensor_y = torch.tensor(train_data_y).long()\n",
    "            train_dataset = torch_data.TensorDataset(tensor_x,tensor_y) \n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False)\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                # Erase accumulated gradients\n",
    "                optimizer.zero_grad() # Forward pass\n",
    "                output = model(data) # Calculate loss\n",
    "                loss = loss_fn(output, target) # Backward pass\n",
    "                loss.backward() # Weight update\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            tensor_x = torch.Tensor(val_data_x) # transform to torch tensor\n",
    "            tensor_y = torch.tensor(val_data_y).long()\n",
    "            test_dataset = torch_data.TensorDataset(tensor_x,tensor_y) \n",
    "            \n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "            test_model(model, train_loader, \"Training set\")\n",
    "            test_model(model, test_loader, \"Validation set\")\n",
    "                \n",
    "        # Track loss each epoch\n",
    "        # print('Train Epoch: %d Loss: %.4f' % (epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_model(model, optimizer, 1, 97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, set_name):\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('%s: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (set_name, test_loss, correct, len(test_loader.dataset),\n",
    "           100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
